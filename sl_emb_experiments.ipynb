{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sl-emb-experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "De5gTAW4zh3Z",
        "g7XDRFZCFN8g",
        "8YlBViaEIaJe"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R48l3SNmExIX"
      },
      "source": [
        "# Schema Linking Embeddings: Running Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De5gTAW4zh3Z"
      },
      "source": [
        "## Download Necessary Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87T5BetZQYQv"
      },
      "source": [
        "!pip -q install fasttext\n",
        "!git clone https://github.com/geokats/schema-linking-embeddings.git\n",
        "!pip -q install -r schema-linking-embeddings/requirements.txt\n",
        "!tar xvjf /content/schema-linking-embeddings/wikisql/data.tar.bz2 -C /content/schema-linking-embeddings/wikisql/\n",
        "!wget https://raw.githubusercontent.com/facebookresearch/fastText/master/python/doc/examples/bin_to_vec.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7XDRFZCFN8g"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoGzkV09FPgH"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "import json\n",
        "import argparse\n",
        "import random\n",
        "import sys\n",
        "import pandas as pd\n",
        "from gensim.models import KeyedVectors\n",
        "from tqdm.auto import tqdm\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "os.chdir(\"/content/schema-linking-embeddings/\")\n",
        "from alignment.align import align_embeddings\n",
        "from util import wikisql_table_to_df, create_table_emb, vector_align, create_gt\n",
        "from util import get_row_matches, get_col_matches, add_aligned_vectors, get_stats, get_rec, get_prec, get_f_score\n",
        "os.chdir(\"/content/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoyO8hfzLm7J"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YlBViaEIaJe"
      },
      "source": [
        "## Prepare Schema Linking Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZYQH_IPIdec"
      },
      "source": [
        "os.chdir(\"/content/schema-linking-embeddings/\")\n",
        "\n",
        "!python create_data.py \\\n",
        "  -i /content/schema-linking-embeddings/wikisql/data/train.jsonl \\\n",
        "  -t /content/schema-linking-embeddings/wikisql/data/train.tables.jsonl \\\n",
        "  -o /content/schema-linking-embeddings/wikisql/data/train.sl.jsonl \n",
        "\n",
        "!python create_data.py \\\n",
        "  -i /content/schema-linking-embeddings/wikisql/data/dev.jsonl \\\n",
        "  -t /content/schema-linking-embeddings/wikisql/data/dev.tables.jsonl \\\n",
        "  -o /content/schema-linking-embeddings/wikisql/data/dev.sl.jsonl \n",
        "\n",
        "os.chdir(\"/content/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEREfj5_FWKi"
      },
      "source": [
        "## Define Experiment's Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX7XjrpKFZUc"
      },
      "source": [
        "EMBEDDING_ALG = \"fasttext\"\n",
        "EMBEDDING_DIM = 100\n",
        "COL_THRESHOLD = 0.5\n",
        "ROW_THRESHOLD = 0.6\n",
        "LOAD_QUERIES = 1000\n",
        "\n",
        "TEMP_DIR = \"/content/tmp\"\n",
        "if not os.path.isdir(TEMP_DIR):\n",
        "  !mkdir {TEMP_DIR}\n",
        "\n",
        "#Initialize statistics\n",
        "col_stats = {\n",
        "    'tab_tp' : 0, 'tab_fp' : 0, 'tab_fn' : 0,\n",
        "    'pre_tp' : 0, 'pre_fp' : 0, 'pre_fn' : 0,\n",
        "}\n",
        "\n",
        "row_stats = {\n",
        "    'tab_tp' : 0, 'tab_fp' : 0, 'tab_fn' : 0,\n",
        "    'pre_tp' : 0, 'pre_fp' : 0, 'pre_fn' : 0,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFX1A-HGE8UA"
      },
      "source": [
        "## Download Pre-trained Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt1UfsWvNU7O"
      },
      "source": [
        "if EMBEDDING_ALG == 'fasttext':\n",
        "  if not os.path.isfile(\"/content/cc.en.300.bin\"):\n",
        "    #Download file if we haven't already\n",
        "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "    !gzip -d /content/cc.en.300.bin.gz\n",
        "  \n",
        "  if not os.path.isfile(f\"/content/cc.en.{EMBEDDING_DIM}.bin\"):\n",
        "    #Resize file if necessary\n",
        "    ft = fasttext.load_model('/content/cc.en.300.bin')\n",
        "    fasttext.util.reduce_model(ft, EMBEDDING_DIM)\n",
        "    pre_trained_emb_file = f\"/content/cc.en.{EMBEDDING_DIM}.bin\"\n",
        "    ft.save_model(pre_trained_emb_file)\n",
        "  \n",
        "  pre_trained_emb_file = f\"/content/cc.en.{EMBEDDING_DIM}.bin\"\n",
        "\n",
        "  if pre_trained_emb_file.endswith(\".bin\"):\n",
        "    #Convert .bin file to .vec\n",
        "    new_name = pre_trained_emb_file.replace(\".bin\", \".vec\")\n",
        "    !python bin_to_vec.py {pre_trained_emb_file} > {new_name}\n",
        "    pre_trained_emb_file = new_name\n",
        "\n",
        "elif EMBEDDING_ALG == 'word2vec':\n",
        "  if EMBEDDING_DIM == 100:\n",
        "    if not os.path.isfile(\"/content/enwiki_20180420_100d.txt.bz2\"):\n",
        "      #Download file if we haven't already\n",
        "      !wget http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/enwiki_20180420_100d.txt.bz2\n",
        "      !tar -xvf /content/enwiki_20180420_100d.txt.bz2\n",
        "      pre_trained_emb_file = f\"/content/enwiki_20180420_100d.txt\"\n",
        "  \n",
        "  elif EMBEDDING_DIM == 300:\n",
        "    if not os.path.isfile(\"/content/enwiki_20180420_300d.txt.bz2\"):\n",
        "      #Download file if we haven't already\n",
        "      !wget http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/enwiki_20180420_300d.txt.bz2\n",
        "      !tar -xvf /content/enwiki_20180420_300d.txt.bz2\n",
        "      pre_trained_emb_file = f\"/content/enwiki_20180420_300d.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbPY77BGFHLN"
      },
      "source": [
        "## Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S24VGO2tOx1G"
      },
      "source": [
        "#Load data\n",
        "unique_words = set()\n",
        "\n",
        "queries_per_table = {}\n",
        "with open(\"/content/schema-linking-embeddings/wikisql/data/train.sl.jsonl\", 'r') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        obj = json.loads(line)\n",
        "        tid = obj['table_id']\n",
        "        if tid in queries_per_table:\n",
        "          queries_per_table[tid].append(obj)\n",
        "        else:\n",
        "          queries_per_table[tid] = [obj]\n",
        "\n",
        "        #Add all words to unique words\n",
        "        words = obj['question']\n",
        "        for word in words:\n",
        "              unique_words.add(word)\n",
        "\n",
        "        if i + 1 == LOAD_QUERIES:\n",
        "          break\n",
        "\n",
        "tables = {}\n",
        "with open(\"/content/schema-linking-embeddings/wikisql/data/train.tables.jsonl\", 'r') as f:\n",
        "    for line in f:\n",
        "        obj = json.loads(line)\n",
        "        if obj['id'] not in queries_per_table:\n",
        "          continue\n",
        "\n",
        "        tables[obj['id']] = obj\n",
        "        \n",
        "        #Add all words to unique words\n",
        "        for h in obj['header']:\n",
        "          words = nltk.word_tokenize(h)\n",
        "          for word in words:\n",
        "            unique_words.add(word)\n",
        "        for row in obj['rows']:\n",
        "          for c in row:\n",
        "            words = nltk.word_tokenize(str(c))\n",
        "            for word in words:\n",
        "              unique_words.add(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsd8nI1BHFUD"
      },
      "source": [
        "new_emb_file = pre_trained_emb_file + \".lite\"\n",
        "\n",
        "lines = []\n",
        "\n",
        "with open(pre_trained_emb_file, 'r') as inpf:\n",
        "  for i, line in enumerate(inpf):\n",
        "    if i == 0:\n",
        "      continue\n",
        "    else:\n",
        "      l = line.split(' ')\n",
        "      if l[0] in unique_words:\n",
        "        lines.append(line)\n",
        "\n",
        "with open(new_emb_file, 'w') as outf:\n",
        "  outf.write(f\"{len(lines)} {EMBEDDING_DIM}\\n\")\n",
        "  for line in lines:\n",
        "    outf.write(line)\n",
        "    \n",
        "pre_trained_emb_file = new_emb_file\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJkH3E76OgPp"
      },
      "source": [
        "#Load pre-trained embeddings\n",
        "kv_pre = KeyedVectors.load_word2vec_format(pre_trained_emb_file)\n",
        "vec_pre = kv_pre.vectors\n",
        "words_pre_set = set(kv_pre.key_to_index.keys())\n",
        "idx_pre = kv_pre.key_to_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ3J0Ls8PmhV"
      },
      "source": [
        "for table_id, queries in tqdm(queries_per_table.items()):\n",
        "  table = tables[table_id]\n",
        "  tdf = wikisql_table_to_df(table)\n",
        "  \n",
        "  #Create local table embeddings\n",
        "  create_table_emb(tdf, os.path.join(TEMP_DIR, \"tmp.emb\"), EMBEDDING_ALG, EMBEDDING_DIM, TEMP_DIR)\n",
        "  kv_tab = KeyedVectors.load_word2vec_format(os.path.join(TEMP_DIR, \"tmp.emb\"))\n",
        "  vec_tab = kv_tab.vectors\n",
        "  words_tab_set = set(kv_tab.key_to_index.keys())\n",
        "  idx_tab = kv_tab.key_to_index\n",
        "\n",
        "  #Find anchor words\n",
        "  anchors = words_tab_set & words_pre_set #Common words are the intersection\n",
        "  pairs = [(idx_pre[w], idx_tab[w]) for w in anchors]\n",
        "  \n",
        "  #Find alignment matrix of pre-trained vectors to the table embedding space\n",
        "  R = align_embeddings(vec_pre, vec_tab, pairs)\n",
        "\n",
        "  for query in queries:\n",
        "    #Construct ground truths\n",
        "    gt_col_matches, gt_row_matches = create_gt(query['sql'], query['schema_links'], tdf)\n",
        "    #Tokenize NLQ\n",
        "    nlq = query['question']\n",
        "    tokens = nltk.word_tokenize(nlq)\n",
        "    # #Align NLQ pre-trained vectors to local table embeddings\n",
        "    kv = add_aligned_vectors(kv_tab, tokens, vec_pre, idx_pre, R)\n",
        "\n",
        "    #Predictions for columns by localy aligned embeddings\n",
        "    col_matches = get_col_matches(tokens, tdf, kv_tab, threshold=COL_THRESHOLD)\n",
        "    tp, fp, fn = get_stats(col_matches, gt_col_matches)\n",
        "    col_stats['tab_tp'] += tp\n",
        "    col_stats['tab_fp'] += fp\n",
        "    col_stats['tab_fn'] += fn\n",
        "    #Predictions for rows by localy aligned embeddings\n",
        "    row_matches = get_row_matches(tokens, tdf, kv_tab, threshold=COL_THRESHOLD)\n",
        "    tp, fp, fn = get_stats(row_matches, gt_row_matches)\n",
        "    row_stats['tab_tp'] += tp\n",
        "    row_stats['tab_fp'] += fp\n",
        "    row_stats['tab_fn'] += fn\n",
        "\n",
        "    #Predictions for columns by pre-trained embeddings\n",
        "    col_matches = get_col_matches(tokens, tdf, kv_pre, threshold=ROW_THRESHOLD)\n",
        "    tp, fp, fn = get_stats(col_matches, gt_col_matches)\n",
        "    col_stats['pre_tp'] += tp\n",
        "    col_stats['pre_tp'] += fp\n",
        "    col_stats['pre_fn'] += fn\n",
        "    #Predictions for rows by pre-trained embeddings\n",
        "    row_matches = get_row_matches(tokens, tdf, kv_pre, threshold=ROW_THRESHOLD)\n",
        "    tp, fp, fn = get_stats(row_matches, gt_row_matches)\n",
        "    row_stats['pre_tp'] += tp\n",
        "    row_stats['pre_fp'] += fp\n",
        "    row_stats['pre_fn'] += fn       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkI2oarZDYRj"
      },
      "source": [
        "tab_col_prec, pre_col_prec = get_prec(col_stats)\n",
        "tab_col_rec, pre_col_rec = get_rec(col_stats)\n",
        "tab_col_f = get_f_score(tab_col_prec, tab_col_rec)\n",
        "pre_col_f = get_f_score(pre_col_prec, pre_col_rec)\n",
        "\n",
        "tab_row_prec, pre_row_prec = get_prec(row_stats)\n",
        "tab_row_rec, pre_row_rec = get_rec(row_stats)\n",
        "tab_row_f = get_f_score(tab_row_prec, tab_row_rec)\n",
        "pre_row_f = get_f_score(pre_row_prec, pre_row_rec)\n",
        "\n",
        "print(f\"Locally Aligned col prec: {tab_col_prec}\")\n",
        "print(f\"Locally Aligned col rec: {tab_col_rec}\")\n",
        "print(f\"Locally Aligned col f1: {tab_col_f}\\n\")\n",
        "\n",
        "print(f\"Locally Aligned row prec: {tab_row_prec}\")\n",
        "print(f\"Locally Aligned row rec: {tab_row_rec}\")\n",
        "print(f\"Locally Aligned row f1: {tab_row_f}\\n\")\n",
        "\n",
        "print(f\"Pre-trained col prec: {pre_col_prec}\")\n",
        "print(f\"Pre-trained col rec: {pre_col_rec}\")\n",
        "print(f\"Pre-trained col f1: {pre_col_f}\\n\")\n",
        "\n",
        "print(f\"Pre-trained row prec: {pre_row_prec}\")\n",
        "print(f\"Pre-trained row rec: {pre_row_rec}\")\n",
        "print(f\"Pre-trained row f1: {pre_row_f}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}